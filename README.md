# NBA Statistics Scraper - Distributed RAG System# ğŸ€ Distributed RAG-Based NBA Stats Scraper# Distributed RAG-Based Web Scraper Framework



A production-grade distributed system for NBA statistics scraping, processing, and AI-powered analysis. Built with microservices architecture using Kafka, MongoDB, ChromaDB, and GPT-4.



## ğŸ¯ Quick StartA **production-grade distributed web scraping system** with RAG (Retrieval-Augmented Generation) capabilities for NBA statistics. Built with Kafka, MongoDB, ChromaDB, and OpenAI GPT-4.A distributed web scraping system with RAG (Retrieval-Augmented Generation) capabilities for scraping and analyzing NBA player statistics.



### Prerequisites

- Python 3.12+, Docker, Node.js 18+

- OpenAI API key[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)## ğŸ¯ Target Use Case



### Setup (5 minutes)[![FastAPI](https://img.shields.io/badge/FastAPI-0.120.2-green.svg)](https://fastapi.tiangolo.com/)Scraping NBA player statistics tables from `https://www.nba.com/stats/alltime-leaders` and similar structured data pages, with intelligent querying via natural language.

```bash

# 1. Clone and configure[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

git clone <repo-url> && cd DS-MT

cp .env.example .env  # Add your OpenAI API key## ğŸ—ï¸ Architecture



# 2. Start Docker infrastructure## ğŸŒŸ Features

docker-compose up -d

### Tech Stack

# 3. Setup Python backend

python3 -m venv venv && source venv/bin/activate- âš¡ **Distributed Scraping**: Kafka-based event-driven architecture with horizontal scaling- **Distributed Computing**: Ray

pip install -r requirements.txt

- ğŸ¤– **AI-Powered Queries**: Natural language queries using OpenAI GPT-4 and vector search- **Message Queue**: Apache Kafka

# 4. Start services (3 terminals)

# Terminal 1: API Server- ğŸ”„ **Fault Tolerance**: Exponential backoff retry logic and dead letter queue- **Databases**: 

python -m uvicorn api.main:app --reload --host 0.0.0.0 --port 8000

- ğŸ“Š **RESTful API**: 15+ endpoints for data access and system management  - MongoDB (raw and processed data storage)

# Terminal 2: Kafka Workers

./scripts/start_workers.sh- ğŸ“ˆ **Monitoring**: Comprehensive metrics (CPU, memory, database stats)  - ChromaDB (vector embeddings)



# Terminal 3: React UI- ğŸ§ª **Tested**: Complete E2E test suite with 100% pass rate- **LLM**: OpenAI API

cd ui && npm install && npm run dev

```- ğŸ³ **Containerized**: Docker Compose for easy deployment- **Web Framework**: FastAPI



### Access- **Scraping**: Scrapy, Selenium, Playwright

- **Web UI**: http://localhost:5173

- **API Docs**: http://localhost:8000/docs## ğŸš€ Quick Start- **Deployment**: Docker Compose

- **Health Check**: http://localhost:8000/api/v1/health



## ğŸ—ï¸ System Architecture

```bash### Project Structure

```

React UI (5173) â†’ FastAPI (8000) â†’ MongoDB + ChromaDB + Kafka# 1. Clone and setup```

                                   â†’ Scraper/Processor Workers

                                   â†’ OpenAI GPT-4git clone https://github.com/iYEiD/ds-midterm.gitdistributed-rag-scraper/

```

cd DS-MTâ”œâ”€â”€ scraper/              # Web scraping module

**6 UI Pages**: Dashboard | Search | RAG Query | Leaders | Submit Job | System Health

â”œâ”€â”€ processor/            # Data processing module

## ğŸ“Š Current System Status

# 2. Start infrastructureâ”œâ”€â”€ rag/                  # RAG and LLM integration

- âœ… 50 NBA players with complete career stats

- âœ… 50 vector embeddings for semantic searchdocker-compose up -dâ”œâ”€â”€ api/                  # FastAPI service

- âœ… RAG-powered AI queries with GPT-4

- âœ… Real-time system monitoringâ”œâ”€â”€ infrastructure/       # Kafka, MongoDB configs

- âœ… < 2s query response time

# 3. Setup Python environmentâ”œâ”€â”€ docker/               # Dockerfiles

## ğŸ”§ Management

python -m venv venvâ”œâ”€â”€ tests/                # Unit and integration tests

**Workers:**

```bashsource venv/bin/activateâ”œâ”€â”€ docs/                 # Documentation and screenshots

./scripts/start_workers.sh   # Start scraper + processor

./scripts/stop_workers.sh    # Stop all workerspip install -r requirements.txtâ”œâ”€â”€ docker-compose.yml    # Local infrastructure

tail -f logs/scraper_worker.log  # View logs

```â”œâ”€â”€ requirements.txt      # Python dependencies



**Troubleshooting:**# 4. Start API serverâ””â”€â”€ README.md            # This file

```bash

docker-compose restart       # Restart infrastructureuvicorn api.main:app --host 0.0.0.0 --port 8000```

docker-compose logs -f       # View container logs

pytest tests/ -v             # Run test suite

```

# 5. Test the system## ğŸš€ Quick Start

## ğŸ“š Documentation

curl http://localhost:8000/api/v1/health

- **[DEPLOYMENT_GUIDE.md](DEPLOYMENT_GUIDE.md)** - Complete setup, infrastructure, troubleshooting

- **[docs/FINAL_PROJECT_REPORT.md](docs/FINAL_PROJECT_REPORT.md)** - Project report with architecture details```### Prerequisites

- **[REQUIREMENTS_CHECKLIST.md](REQUIREMENTS_CHECKLIST.md)** - Requirements tracking

- Python 3.10+

## ğŸ› ï¸ Tech Stack

## ğŸ“š Documentation- Docker and Docker Compose

**Backend**: Python 3.12, FastAPI, Selenium, Kafka, MongoDB, ChromaDB, GPT-4  

**Frontend**: React 19, Vite, Axios, Recharts, React Router  - OpenAI API Key

**Infrastructure**: Docker, Docker Compose

- **[Complete Project Report](docs/PROJECT_REPORT.md)** - Comprehensive system documentation

## ğŸ“‹ Key Features

- **[Testing Guide](TESTING_GUIDE.md)** - How to test the distributed system### 1. Clone and Setup Virtual Environment

- Distributed web scraping with Selenium (headless Chrome)

- Kafka message queues for async processing- **[API Documentation](http://localhost:8000/api/v1/docs)** - Interactive API docs (when server running)```bash

- Vector embeddings with ChromaDB for semantic search

- RAG system with GPT-4 for natural language queries# Clone the repository

- Real-time monitoring dashboard

- RESTful API with 15+ endpoints## ğŸ—ï¸ Architecturegit clone <your-repo-url>

- Responsive React UI with data visualization

cd DS-MT

## âš ï¸ Notes

```

- **Development setup** - MongoDB has no auth, CORS allows localhost

- **API Key required** - Add `OPENAI_API_KEY` to `.env`User â†’ Nginx â†’ FastAPI â†’ Orchestrator â†’ Kafka Queue# Create virtual environment

- **Resource usage** - ~2.5 GB RAM, 30% CPU during operation

                   â†“                          â†“python -m venv venv

---

                MongoDB â†â”€ Scraper Workers â†â”€â”€â”˜source venv/bin/activate  # On Windows: venv\Scripts\activate

**Version**: 1.0 | **Status**: Production Ready âœ… | **Last Updated**: Oct 31, 2025

                   â†“              â†“```

              ChromaDB â†â”€ Processor Workers â†â”€â”€ Kafka Results

                   â†“### 2. Install Dependencies

              RAG System (GPT-4) â†’ API Response```bash

```pip install -r requirements.txt

```

## ğŸ’» Usage Examples

### 3. Configure Environment Variables

### Natural Language Queries (RAG)```bash

```bashcp .env.example .env

curl -X POST http://localhost:8000/api/v1/query \# Edit .env and add your OpenAI API key and other configurations

  -H "Content-Type: application/json" \```

  -d '{"query": "Who is the all-time leading scorer?", "top_k": 5}'

```### 4. Start Infrastructure Services

```bash

### Get Leaders by Categorydocker-compose up -d

```bash```

curl "http://localhost:8000/api/v1/stats/leaders?category=PTS&limit=10"

```This will start:

- MongoDB (port 27017)

### Submit Scraping Job- Apache Kafka (port 9092)

```bash- Zookeeper (port 2181)

python orchestrator.py --urls "https://www.nba.com/stats/alltime-leaders?StatCategory=REB" --monitor- Ray Head Node (dashboard on port 8265)

```

### 5. Verify Services

## ğŸ§ª Testing```bash

# Check all containers are running

```bashdocker-compose ps

# Run comprehensive E2E test

python tests/test_comprehensive_e2e.py# Access Ray dashboard

open http://localhost:8265

# Test API endpoints

./test_api.sh# Test MongoDB connection

python -c "from pymongo import MongoClient; client = MongoClient('mongodb://localhost:27017/'); print('MongoDB connected:', client.server_info()['version'])"

# Test distributed workers```

python tests/test_distributed_e2e.py

```## ğŸ“‹ Development Phases



## ğŸ› ï¸ Technologies- [x] **Phase 1**: Environment & Repository Setup

- [ ] **Phase 2**: Local Infrastructure with Docker Compose

**Core**: Python 3.12, FastAPI, Apache Kafka 7.5, MongoDB 7.0, ChromaDB  - [ ] **Phase 3**: Web Scraping Module Development

**AI/ML**: OpenAI GPT-4, text-embedding-3-small (1536 dimensions)  - [ ] **Phase 4**: Data Processing Module

**Scraping**: Scrapy 2.13, Playwright 1.55, BeautifulSoup  - [ ] **Phase 5**: RAG Integration with ChromaDB and OpenAI

**Monitoring**: psutil, loguru, prometheus-client  - [ ] **Phase 6**: FastAPI Service Development

**Deployment**: Docker, Docker Compose, Nginx- [ ] **Phase 7**: Load Balancing & Fault Tolerance

- [ ] **Phase 8**: Integration Testing & Documentation

## ğŸ“Š Current Dataset

## ğŸ§ª Testing

- **50 NBA Players** with comprehensive career statistics

- **6 Statistical Categories**: PTS, AST, REB, STL, BLK, etc.```bash

- **50 Vector Embeddings** (1536 dimensions each)# Run all tests

- **Source**: NBA.com official statisticspytest



## ğŸ“ Project Structure# Run with coverage

pytest --cov=. --cov-report=html

```

DS-MT/# Run specific test module

â”œâ”€â”€ api/                  # FastAPI REST APIpytest tests/test_scraper.py

â”œâ”€â”€ scraper/              # Distributed web scraping```

â”œâ”€â”€ processor/            # Data processing pipeline

â”œâ”€â”€ rag/                  # RAG system (vector store, LLM)## ğŸ“Š API Documentation

â”œâ”€â”€ monitoring/           # Metrics and health checks

â”œâ”€â”€ infrastructure/       # Nginx, configsOnce the API is running:

â”œâ”€â”€ tests/                # E2E test suites```bash

â”œâ”€â”€ docs/                 # Documentationuvicorn api.main:app --reload

â”œâ”€â”€ orchestrator.py       # Job coordinator```

â””â”€â”€ docker-compose.yml    # Infrastructure services

```Access interactive API documentation at:

- Swagger UI: http://localhost:8000/docs

## ğŸ¯ Key Features- ReDoc: http://localhost:8000/redoc



### 1. Distributed Architecture## ğŸ”§ Development Tips

- Kafka consumer groups for automatic load balancing

- Horizontal scaling (add more workers dynamically)1. **Incremental Development**: Complete and test each phase before moving forward

- No single point of failure2. **Frequent Commits**: Commit after each working feature

3. **Test Early**: Don't accumulate untested code

### 2. RAG System4. **Monitor Resources**: Watch Docker container resources via `docker stats`

1. User query â†’ OpenAI embedding5. **API Keys**: Never commit `.env` file to Git

2. ChromaDB similarity search â†’ Top-K results

3. Context + query â†’ GPT-4 prompt## ğŸ“ Documentation

4. Natural language answer with sources

Detailed documentation for each phase can be found in:

### 3. Fault Tolerance- `docs/report.md` - Complete implementation report with screenshots

- Exponential backoff retry (3 attempts)- `docs/architecture.md` - System architecture and flowcharts

- Dead letter queue for permanent failures- API documentation - Auto-generated via FastAPI

- Health monitoring and graceful degradation

## ğŸ› Troubleshooting

## ğŸ“ˆ Performance

### Kafka Connection Issues

- **Scraping**: 5-10 pages/minute```bash

- **Processing**: 20-30 seconds (URL â†’ embeddings)# Check Kafka logs

- **API Latency**: 50-200ms (database queries)docker-compose logs kafka

- **RAG Queries**: 2-5 seconds (includes OpenAI)

- **Scalability**: Tested with 10 parallel workers# List topics

docker exec -it <kafka-container> kafka-topics --list --bootstrap-server localhost:9092

## ğŸ› Troubleshooting```



```bash### MongoDB Issues

# Check services```bash

docker-compose ps# Check MongoDB logs

docker-compose logs mongodb

# View logs

tail -f api_server.log# Access MongoDB shell

docker exec -it <mongodb-container> mongosh

# Test Kafka```

docker exec -it kafka kafka-topics --list --bootstrap-server localhost:9092

### Ray Issues

# Test MongoDB```bash

mongosh mongodb://localhost:27017/nba_scraper# Check Ray dashboard

```open http://localhost:8265



## ğŸ“š Additional Resources# View Ray logs

docker-compose logs ray-head

- [Original Project Plan](rag_scraper_plan.md)```

- [API Testing Script](test_api.sh)

- [Nginx Config](infrastructure/nginx.conf)## ğŸ“„ License



## ğŸ‘¤ AuthorThis project is developed as part of a Data Science course midterm project.



**iYEiD**  ## ğŸ¤ Contributing

GitHub: [@iYEiD](https://github.com/iYEiD)  

Repository: [ds-midterm](https://github.com/iYEiD/ds-midterm)This is an academic project. For questions or suggestions, please contact the project maintainer.



## ğŸ“„ License## ğŸ“ Support



MIT License - see LICENSE file for detailsFor issues and questions:

1. Check the `docs/troubleshooting.md` guide

---2. Review the project plan in `rag_scraper_plan.md`

3. Open an issue in the repository

**Status**: âœ… Production Ready | ğŸ§ª Fully Tested | ğŸ“Š 50 Players | ğŸ¤– AI-Powered

**Built for Data Science Midterm Project - October 2025**
